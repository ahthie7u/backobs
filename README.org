#+STARTUP: hidestars
#+STARTUP: indent

#+author: F. Dangel

* Combining [[https://deepobs.readthedocs.io/en/stable/][DeepOBS]] and [[https://backpack.readthedocs.io/en/latest/][BackPACK]]
For an overview about the supported problems, run
#+BEGIN_SRC bash :results output
  python example/supported.py 
#+END_SRC

#+RESULTS:
#+begin_example
Supported:
	✔ cifar10_3c3d
	✔ cifar10_vgg16
	✔ cifar10_vgg19
	✔ cifar100_3c3d
	✔ cifar100_allcnnc
	✔ cifar100_vgg16
	✔ cifar100_vgg19
	✔ fmnist_2c2d
	✔ fmnist_logreg
	✔ fmnist_mlp
	✔ mnist_2c2d
	✔ mnist_logreg
	✔ mnist_mlp
	✔ quadratic_deep
	✔ svhn_3c3d
Not supported:
	❌ cifar100_wrn164
	❌ cifar100_wrn404
	❌ fmnist_vae
	❌ mnist_vae
	❌ svhn_wrn164
#+end_example

Unsupported problems include:
- Variational autoencoder problems (~mnist_vae~, ~fmnist-vae~)
- Problems with batch normalization (~cifar100_wrn164~, ~cifar100_wrn404~, ~svhn_wrn164~)

** Installation
#+BEGIN_SRC bash
pip install -e git://github.com/f-dangel/backobs.git@master#egg=backobs
#+END_SRC
** Basic examples
- How to extend a testproblem in [[https://github.com/fsschneider/DeepOBS][DeepOBS]] with [[https://www.backpack.pt][BackPACK]] ([[file:./example/extend.py][file]]):
  #+BEGIN_SRC bash :results output
    python example/extend.py
  #+END_SRC

  #+RESULTS:
#+begin_example
dense.weight
	.grad.shape:              torch.Size([10, 784])
	.grad_batch.shape:        torch.Size([128, 10, 784])
	.variance.shape:          torch.Size([10, 784])
	.sum_grad_squared.shape:  torch.Size([10, 784])
	.batch_l2.shape:          torch.Size([128])
	.diag_ggn_mc.shape:       torch.Size([10, 784])
	.diag_ggn_exact.shape:    torch.Size([10, 784])
	.diag_h.shape:            torch.Size([10, 784])
	.kfac (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]
	.kflr (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]
	.kfra (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]
dense.bias
	.grad.shape:              torch.Size([10])
	.grad_batch.shape:        torch.Size([128, 10])
	.variance.shape:          torch.Size([10])
	.sum_grad_squared.shape:  torch.Size([10])
	.batch_l2.shape:          torch.Size([128])
	.diag_ggn_mc.shape:       torch.Size([10])
	.diag_ggn_exact.shape:    torch.Size([10])
	.diag_h.shape:            torch.Size([10])
	.kfac (shapes):           [torch.Size([10, 10])]
	.kflr (shapes):           [torch.Size([10, 10])]
	.kfra (shapes):           [torch.Size([10, 10])]
#+end_example
- How to extend a testproblem with [[https://www.backpack.pt][BackPACK]] and get access to the unreduced loss ([[file:./example/extend_with_access_unreduced_loss.py][here]]):
  #+BEGIN_SRC bash :results output
    python example/extend_with_access_unreduced_loss.py
  #+END_SRC

  #+RESULTS:
  #+begin_example
  Individual loss shape:    torch.Size([128])
  Mini-batch loss:          tensor(2.3026, device='cuda:0', grad_fn=<AddBackward0>)
  Averaged individual loss: tensor(2.3026, device='cuda:0')
  dense.weight
    .grad.shape:              torch.Size([10, 784])
    .grad_batch.shape:        torch.Size([128, 10, 784])
    .variance.shape:          torch.Size([10, 784])
    .sum_grad_squared.shape:  torch.Size([10, 784])
    .batch_l2.shape:          torch.Size([128])
    .diag_ggn_mc.shape:       torch.Size([10, 784])
    .diag_ggn_exact.shape:    torch.Size([10, 784])
    .diag_h.shape:            torch.Size([10, 784])
    .kfac (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]
    .kflr (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]
    .kfra (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]
  dense.bias
    .grad.shape:              torch.Size([10])
    .grad_batch.shape:        torch.Size([128, 10])
    .variance.shape:          torch.Size([10])
    .sum_grad_squared.shape:  torch.Size([10])
    .batch_l2.shape:          torch.Size([128])
    .diag_ggn_mc.shape:       torch.Size([10])
    .diag_ggn_exact.shape:    torch.Size([10])
    .diag_h.shape:            torch.Size([10])
    .kfac (shapes):           [torch.Size([10, 10])]
    .kflr (shapes):           [torch.Size([10, 10])]
    .kfra (shapes):           [torch.Size([10, 10])]
  #+end_example
- A [[https://github.com/fsschneider/DeepOBS][DeepOBS]] testproblem runner for SGD, extended with [[https://www.backpack.pt][BackPACK]] functionality ([[file:./example/runner.py][here]]):
  #+BEGIN_SRC bash :results output
    python example/run.py mnist_logreg --lr 0.1
  #+END_SRC

  #+RESULTS:
  #+begin_example
  ,********************************
  Evaluating after 0 of 1 epochs...
  TRAIN: loss 2.30259, acc 0.104610
  VALID: loss 2.30259, acc 0.100510
  TEST: loss 2.30259, acc 0.098010
  ,********************************
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 10, 784)
    Parameter 1: Shape (3, 10)
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 10, 784)
    Parameter 1: Shape (3, 10)
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 10, 784)
    Parameter 1: Shape (3, 10)
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 10, 784)
    Parameter 1: Shape (3, 10)
  ,********************************
  Evaluating after 1 of 1 epochs...
  TRAIN: loss 2.83062, acc 0.176418
  VALID: loss 2.81457, acc 0.180918
  TEST: loss 2.84024, acc 0.178218
  ,********************************
  #+end_example

** Important
- ℓ₂ regularization is *not supported*:
  #+BEGIN_SRC bash :results output
    # ℓ₂ not supported: this will crash!
    # python example/run.py cifar10_3c3d --lr 0.1

    # ℓ₂ disabled: works
    python example/run.py cifar10_3c3d --lr 0.1 --l2_reg 0.0
  #+END_SRC

  #+RESULTS:
  #+begin_example
  Files already downloaded and verified
  Files already downloaded and verified
  Files already downloaded and verified
  ,********************************
  Evaluating after 0 of 1 epochs...
  TRAIN: loss 2.32498, acc 0.101810
  VALID: loss 2.32938, acc 0.096710
  TEST: loss 2.32596, acc 0.100110
  ,********************************
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 64, 3, 5, 5)
    Parameter 1: Shape (3, 64)
    Parameter 2: Shape (3, 96, 64, 3, 3)
    Parameter 3: Shape (3, 96)
    Parameter 4: Shape (3, 128, 96, 3, 3)
    Parameter 5: Shape (3, 128)
    Parameter 6: Shape (3, 512, 1152)
    Parameter 7: Shape (3, 512)
    Parameter 8: Shape (3, 256, 512)
    Parameter 9: Shape (3, 256)
    Parameter 10: Shape (3, 10, 256)
    Parameter 11: Shape (3, 10)
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 64, 3, 5, 5)
    Parameter 1: Shape (3, 64)
    Parameter 2: Shape (3, 96, 64, 3, 3)
    Parameter 3: Shape (3, 96)
    Parameter 4: Shape (3, 128, 96, 3, 3)
    Parameter 5: Shape (3, 128)
    Parameter 6: Shape (3, 512, 1152)
    Parameter 7: Shape (3, 512)
    Parameter 8: Shape (3, 256, 512)
    Parameter 9: Shape (3, 256)
    Parameter 10: Shape (3, 10, 256)
    Parameter 11: Shape (3, 10)
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 64, 3, 5, 5)
    Parameter 1: Shape (3, 64)
    Parameter 2: Shape (3, 96, 64, 3, 3)
    Parameter 3: Shape (3, 96)
    Parameter 4: Shape (3, 128, 96, 3, 3)
    Parameter 5: Shape (3, 128)
    Parameter 6: Shape (3, 512, 1152)
    Parameter 7: Shape (3, 512)
    Parameter 8: Shape (3, 256, 512)
    Parameter 9: Shape (3, 256)
    Parameter 10: Shape (3, 10, 256)
    Parameter 11: Shape (3, 10)
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 64, 3, 5, 5)
    Parameter 1: Shape (3, 64)
    Parameter 2: Shape (3, 96, 64, 3, 3)
    Parameter 3: Shape (3, 96)
    Parameter 4: Shape (3, 128, 96, 3, 3)
    Parameter 5: Shape (3, 128)
    Parameter 6: Shape (3, 512, 1152)
    Parameter 7: Shape (3, 512)
    Parameter 8: Shape (3, 256, 512)
    Parameter 9: Shape (3, 256)
    Parameter 10: Shape (3, 10, 256)
    Parameter 11: Shape (3, 10)
  ,********************************
  Evaluating after 1 of 1 epochs...
  TRAIN: loss 2.35165, acc 0.100910
  VALID: loss 2.34386, acc 0.101110
  TEST: loss 2.34491, acc 0.101110
  ,********************************
  #+end_example

* Details and debug info (TODO, ignore everything below)
** Preliminaries 
*** Set up a virtual environment
**** Anaconda
Use the ~.yml~ file to install all required dependencies into a ~conda~ environment named ~backobs~.
#+BEGIN_SRC bash
conda env create -f .conda_env.yml
#+END_SRC
Activate it
#+BEGIN_SRC bash
conda activate backobs
#+END_SRC
**** I don't want to use Anaconda
Make sure you have ~backpack~ and ~deepobs~ installed in your favorite environment.

*** Install ~backobs~
#+BEGIN_SRC bash
pip install -e git://github.com/f-dangel/backobs.git@master#egg=backobs
#+END_SRC
** Example
*** The runner
The [[file:example/][example directory]] contains a [[file:example/runner.py][basic runner]] that integrates BackPACK into DeepOBS test problems. You can modify it to your needs, or leave it as is to run an example.
*** Run SGD on a DeepOBS problem with BackPACK 
There exists a [[file:example/run.py][run script]] you can simply execute from the command line.

Let's run SGD on the MNIST linear regression task with a learning rate of ~0.1~ and momentum of ~0.9~:
#+BEGIN_SRC bash
python example/run.py mnist_logreg --lr 0.1 --momentum 0.9
#+END_SRC

*** Not all problems are supported
ResNets and variational autoencoders are not supported by BackPACK.

For instance, this will crash:
#+BEGIN_SRC 
# WideResNet
python example/run.py svhn_wrn164 --lr 0.1 --momentum 0.9
#+END_SRC

** Tests
*** Reproducing the forward pass manually
Testing [[https://www.backpack.pt][BackPACK]] extensions on a [[https://github.com/fsschneider/DeepOBS][DeepOBS]] problem requires access to the input data. As a first step, we will check here that the forward pass can correctly be reproduced in a manual fashion starting from the mini-batch.

The script ~./test/test_forward.py~ checks for which problems this is possible ($L_2$ regularization is ignored). We check with/without extending the [[https://github.com/fsschneider/DeepOBS][DeepOBS]] problem with [[https://www.backpack.pt][BackPACK]], and with/without adding the regularization term in the forward pass.

Run the following:
#+begin_src bash :results output
  python test/run_test_forward.py
#+end_src 

#+RESULTS:
#+begin_example
✓ [cifar10_3c3d, l2_reg: False, BackPACK: False] DeepOBS: 2.28687, manual: 2.28687
✓ [cifar10_vgg16, l2_reg: False, BackPACK: False] DeepOBS: 2.30151, manual: 2.30151
✓ [cifar10_vgg19, l2_reg: False, BackPACK: False] DeepOBS: 2.30262, manual: 2.30262
✓ [cifar100_3c3d, l2_reg: False, BackPACK: False] DeepOBS: 4.55693, manual: 4.55693
✓ [cifar100_allcnnc, l2_reg: False, BackPACK: False] DeepOBS: 4.56741, manual: 4.56741
✓ [cifar100_vgg16, l2_reg: False, BackPACK: False] DeepOBS: 4.60366, manual: 4.60366
✓ [cifar100_vgg19, l2_reg: False, BackPACK: False] DeepOBS: 4.60555, manual: 4.60555
✓ [cifar100_wrn164, l2_reg: False, BackPACK: False] DeepOBS: 4.31506, manual: 4.31506
✓ [cifar100_wrn404, l2_reg: False, BackPACK: False] DeepOBS: 4.61947, manual: 4.61947
✓ [fmnist_2c2d, l2_reg: False, BackPACK: False] DeepOBS: 2.32473, manual: 2.32473
✓ [fmnist_logreg, l2_reg: False, BackPACK: False] DeepOBS: 2.30259, manual: 2.30259
✓ [fmnist_mlp, l2_reg: False, BackPACK: False] DeepOBS: 2.30591, manual: 2.30591
✓ [fmnist_vae, l2_reg: False, BackPACK: False] DeepOBS: 145.27640, manual: 145.27640
✓ [mnist_2c2d, l2_reg: False, BackPACK: False] DeepOBS: 2.35603, manual: 2.35603
✓ [mnist_logreg, l2_reg: False, BackPACK: False] DeepOBS: 2.30259, manual: 2.30259
✓ [mnist_mlp, l2_reg: False, BackPACK: False] DeepOBS: 2.29524, manual: 2.29524
✓ [mnist_vae, l2_reg: False, BackPACK: False] DeepOBS: 179.56845, manual: 179.56845
❌ [quadratic_deep, l2_reg: False, BackPACK: False] DeepOBS: 5.29617, manual: 4.89908
✓ [svhn_3c3d, l2_reg: False, BackPACK: False] DeepOBS: 2.21970, manual: 2.21970
✓ [svhn_wrn164, l2_reg: False, BackPACK: False] DeepOBS: 1.89063, manual: 1.89063


❌ [cifar10_3c3d, l2_reg: True, BackPACK: False] DeepOBS: 3.54886, manual: 2.28687
❌ [cifar10_vgg16, l2_reg: True, BackPACK: False] DeepOBS: 6.05709, manual: 2.30151
❌ [cifar10_vgg19, l2_reg: True, BackPACK: False] DeepOBS: 6.37784, manual: 2.30262
❌ [cifar100_3c3d, l2_reg: True, BackPACK: False] DeepOBS: 5.94544, manual: 4.55693
❌ [cifar100_allcnnc, l2_reg: True, BackPACK: False] DeepOBS: 4.87410, manual: 4.56741
❌ [cifar100_vgg16, l2_reg: True, BackPACK: False] DeepOBS: 8.40309, manual: 4.60366
❌ [cifar100_vgg19, l2_reg: True, BackPACK: False] DeepOBS: 8.72502, manual: 4.60555
❌ [cifar100_wrn164, l2_reg: True, BackPACK: False] DeepOBS: 4.82936, manual: 4.31506
❌ [cifar100_wrn404, l2_reg: True, BackPACK: False] Raised exception: 'NoneType' object has no attribute 'items'
✓ [fmnist_2c2d, l2_reg: True, BackPACK: False] DeepOBS: 2.32473, manual: 2.32473
✓ [fmnist_logreg, l2_reg: True, BackPACK: False] DeepOBS: 2.30259, manual: 2.30259
✓ [fmnist_mlp, l2_reg: True, BackPACK: False] DeepOBS: 2.30591, manual: 2.30591
✓ [fmnist_vae, l2_reg: True, BackPACK: False] DeepOBS: 145.27640, manual: 145.27640
✓ [mnist_2c2d, l2_reg: True, BackPACK: False] DeepOBS: 2.35603, manual: 2.35603
✓ [mnist_logreg, l2_reg: True, BackPACK: False] DeepOBS: 2.30259, manual: 2.30259
✓ [mnist_mlp, l2_reg: True, BackPACK: False] DeepOBS: 2.29524, manual: 2.29524
✓ [mnist_vae, l2_reg: True, BackPACK: False] DeepOBS: 179.56845, manual: 179.56845
❌ [quadratic_deep, l2_reg: True, BackPACK: False] DeepOBS: 5.29617, manual: 4.89908
❌ [svhn_3c3d, l2_reg: True, BackPACK: False] DeepOBS: 3.48170, manual: 2.21970
❌ [svhn_wrn164, l2_reg: True, BackPACK: False] DeepOBS: 2.37303, manual: 1.89063


✓ [cifar10_3c3d, l2_reg: False, BackPACK: True] DeepOBS: 2.28687, manual: 2.28687
✓ [cifar10_vgg16, l2_reg: False, BackPACK: True] DeepOBS: 2.30151, manual: 2.30151
✓ [cifar10_vgg19, l2_reg: False, BackPACK: True] DeepOBS: 2.30262, manual: 2.30262
✓ [cifar100_3c3d, l2_reg: False, BackPACK: True] DeepOBS: 4.55693, manual: 4.55693
✓ [cifar100_allcnnc, l2_reg: False, BackPACK: True] DeepOBS: 4.56741, manual: 4.56741
✓ [cifar100_vgg16, l2_reg: False, BackPACK: True] DeepOBS: 4.60366, manual: 4.60366
✓ [cifar100_vgg19, l2_reg: False, BackPACK: True] DeepOBS: 4.60555, manual: 4.60555
✓ [cifar100_wrn164, l2_reg: False, BackPACK: True] DeepOBS: 4.31506, manual: 4.31506
✓ [cifar100_wrn404, l2_reg: False, BackPACK: True] DeepOBS: 4.61947, manual: 4.61947
✓ [fmnist_2c2d, l2_reg: False, BackPACK: True] DeepOBS: 2.32473, manual: 2.32473
✓ [fmnist_logreg, l2_reg: False, BackPACK: True] DeepOBS: 2.30259, manual: 2.30259
✓ [fmnist_mlp, l2_reg: False, BackPACK: True] DeepOBS: 2.30591, manual: 2.30591
❌ [fmnist_vae, l2_reg: False, BackPACK: True] Raised exception: 'tuple' object has no attribute 'size'
✓ [mnist_2c2d, l2_reg: False, BackPACK: True] DeepOBS: 2.35603, manual: 2.35603
✓ [mnist_logreg, l2_reg: False, BackPACK: True] DeepOBS: 2.30259, manual: 2.30259
✓ [mnist_mlp, l2_reg: False, BackPACK: True] DeepOBS: 2.29524, manual: 2.29524
❌ [mnist_vae, l2_reg: False, BackPACK: True] Raised exception: 'tuple' object has no attribute 'size'
❌ [quadratic_deep, l2_reg: False, BackPACK: True] DeepOBS: 5.29617, manual: 4.89908
✓ [svhn_3c3d, l2_reg: False, BackPACK: True] DeepOBS: 2.21970, manual: 2.21970
✓ [svhn_wrn164, l2_reg: False, BackPACK: True] DeepOBS: 1.89063, manual: 1.89063


❌ [cifar10_3c3d, l2_reg: True, BackPACK: True] DeepOBS: 3.54886, manual: 2.28687
❌ [cifar10_vgg16, l2_reg: True, BackPACK: True] DeepOBS: 6.05709, manual: 2.30151
❌ [cifar10_vgg19, l2_reg: True, BackPACK: True] DeepOBS: 6.37784, manual: 2.30262
❌ [cifar100_3c3d, l2_reg: True, BackPACK: True] DeepOBS: 5.94544, manual: 4.55693
❌ [cifar100_allcnnc, l2_reg: True, BackPACK: True] DeepOBS: 4.87410, manual: 4.56741
❌ [cifar100_vgg16, l2_reg: True, BackPACK: True] DeepOBS: 8.40309, manual: 4.60366
❌ [cifar100_vgg19, l2_reg: True, BackPACK: True] DeepOBS: 8.72502, manual: 4.60555
❌ [cifar100_wrn164, l2_reg: True, BackPACK: True] DeepOBS: 4.82936, manual: 4.31506
❌ [cifar100_wrn404, l2_reg: True, BackPACK: True] Raised exception: 'NoneType' object has no attribute 'items'
✓ [fmnist_2c2d, l2_reg: True, BackPACK: True] DeepOBS: 2.32473, manual: 2.32473
✓ [fmnist_logreg, l2_reg: True, BackPACK: True] DeepOBS: 2.30259, manual: 2.30259
✓ [fmnist_mlp, l2_reg: True, BackPACK: True] DeepOBS: 2.30591, manual: 2.30591
❌ [fmnist_vae, l2_reg: True, BackPACK: True] Raised exception: 'tuple' object has no attribute 'size'
✓ [mnist_2c2d, l2_reg: True, BackPACK: True] DeepOBS: 2.35603, manual: 2.35603
✓ [mnist_logreg, l2_reg: True, BackPACK: True] DeepOBS: 2.30259, manual: 2.30259
✓ [mnist_mlp, l2_reg: True, BackPACK: True] DeepOBS: 2.29524, manual: 2.29524
❌ [mnist_vae, l2_reg: True, BackPACK: True] Raised exception: 'tuple' object has no attribute 'size'
❌ [quadratic_deep, l2_reg: True, BackPACK: True] DeepOBS: 5.29617, manual: 4.89908
❌ [svhn_3c3d, l2_reg: True, BackPACK: True] DeepOBS: 3.48170, manual: 2.21970
❌ [svhn_wrn164, l2_reg: True, BackPACK: True] DeepOBS: 2.37303, manual: 1.89063


#+end_example
Comments: 
- [ ] *DeepOBS bug?* The forward pass of ~cifar100_wrn404~ does not work with regularization.
- [X] *Variational auto-encoders* mess with IO storing in [[https://www.backpack.pt][BackPACK]], which is expected.
- [ ] *Non-deterministic problem*: The forward pass in ~quadrati_deep~ is not deterministic, with or without [[https://www.backpack.pt][BackPACK]]. At least, the losses only differ between manual/DeepOBS forward pass.
- [X] *Regularized problems* are different from the manual forward pass, as the latter only considers the empirical risk term without regularization. This is expected
*** Checking the loss sum struture
[[https://www.backpack.pt][BackPACK]] assumes the loss to be a sum over individual losses. This excludes ~BatchNorm~. In the test ~test/individual_forward.py~ we check whether the mean of individual losses corresponds to the mini-batch loss.

Run the following to reproduce the results:
#+begin_src bash :results output
  python test/run_test_individual_forward.py
#+end_src 

#+RESULTS:
#+begin_example
✓ [cifar10_3c3d, l2_reg: False, BackPACK: False] DeepOBS: 2.28687, manual for-loop: 2.28687
❌ [cifar10_vgg16, l2_reg: False, BackPACK: False] DeepOBS: 2.30151, manual for-loop: 2.30058, BatchNorm? False, Dropout? True
❌ [cifar10_vgg19, l2_reg: False, BackPACK: False] DeepOBS: 2.30262, manual for-loop: 2.30326, BatchNorm? False, Dropout? True
✓ [cifar100_3c3d, l2_reg: False, BackPACK: False] DeepOBS: 4.55693, manual for-loop: 4.55693
❌ [cifar100_allcnnc, l2_reg: False, BackPACK: False] DeepOBS: 4.56741, manual for-loop: 4.56287, BatchNorm? False, Dropout? True
❌ [cifar100_vgg16, l2_reg: False, BackPACK: False] DeepOBS: 4.60366, manual for-loop: 4.60409, BatchNorm? False, Dropout? True
❌ [cifar100_vgg19, l2_reg: False, BackPACK: False] DeepOBS: 4.60555, manual for-loop: 4.60602, BatchNorm? False, Dropout? True
❌ [cifar100_wrn164, l2_reg: False, BackPACK: False] DeepOBS: 4.31506, manual for-loop: 4.37367, BatchNorm? True, Dropout? False
❌ [cifar100_wrn404, l2_reg: False, BackPACK: False] DeepOBS: 4.61947, manual for-loop: 4.40666, BatchNorm? True, Dropout? False
✓ [fmnist_2c2d, l2_reg: False, BackPACK: False] DeepOBS: 2.32473, manual for-loop: 2.32473
✓ [fmnist_logreg, l2_reg: False, BackPACK: False] DeepOBS: 2.30259, manual for-loop: 2.30259
✓ [fmnist_mlp, l2_reg: False, BackPACK: False] DeepOBS: 2.30591, manual for-loop: 2.30591
❌ [fmnist_vae, l2_reg: False, BackPACK: False] Raised exception: vae_loss_function() missing 2 required positional arguments: 'mean' and 'std_dev'
✓ [mnist_2c2d, l2_reg: False, BackPACK: False] DeepOBS: 2.35603, manual for-loop: 2.35603
✓ [mnist_logreg, l2_reg: False, BackPACK: False] DeepOBS: 2.30259, manual for-loop: 2.30259
✓ [mnist_mlp, l2_reg: False, BackPACK: False] DeepOBS: 2.29524, manual for-loop: 2.29524
❌ [mnist_vae, l2_reg: False, BackPACK: False] Raised exception: vae_loss_function() missing 2 required positional arguments: 'mean' and 'std_dev'
✓ [quadratic_deep, l2_reg: False, BackPACK: False] DeepOBS: 6.66879, manual for-loop: 6.66879
✓ [svhn_3c3d, l2_reg: False, BackPACK: False] DeepOBS: 2.21970, manual for-loop: 2.21970
❌ [svhn_wrn164, l2_reg: False, BackPACK: False] DeepOBS: 1.89063, manual for-loop: 1.84587, BatchNorm? True, Dropout? False


❌ [cifar10_3c3d, l2_reg: True, BackPACK: False] DeepOBS: 3.54886, manual for-loop: 2.28687, BatchNorm? False, Dropout? False
❌ [cifar10_vgg16, l2_reg: True, BackPACK: False] DeepOBS: 6.05709, manual for-loop: 2.30058, BatchNorm? False, Dropout? True
❌ [cifar10_vgg19, l2_reg: True, BackPACK: False] DeepOBS: 6.37784, manual for-loop: 2.30326, BatchNorm? False, Dropout? True
❌ [cifar100_3c3d, l2_reg: True, BackPACK: False] DeepOBS: 5.94544, manual for-loop: 4.55693, BatchNorm? False, Dropout? False
❌ [cifar100_allcnnc, l2_reg: True, BackPACK: False] DeepOBS: 4.87410, manual for-loop: 4.56287, BatchNorm? False, Dropout? True
❌ [cifar100_vgg16, l2_reg: True, BackPACK: False] DeepOBS: 8.40309, manual for-loop: 4.60409, BatchNorm? False, Dropout? True
❌ [cifar100_vgg19, l2_reg: True, BackPACK: False] DeepOBS: 8.72502, manual for-loop: 4.60602, BatchNorm? False, Dropout? True
❌ [cifar100_wrn164, l2_reg: True, BackPACK: False] DeepOBS: 4.82936, manual for-loop: 4.37367, BatchNorm? True, Dropout? False
❌ [cifar100_wrn404, l2_reg: True, BackPACK: False] Raised exception: 'NoneType' object has no attribute 'items'
✓ [fmnist_2c2d, l2_reg: True, BackPACK: False] DeepOBS: 2.32473, manual for-loop: 2.32473
✓ [fmnist_logreg, l2_reg: True, BackPACK: False] DeepOBS: 2.30259, manual for-loop: 2.30259
✓ [fmnist_mlp, l2_reg: True, BackPACK: False] DeepOBS: 2.30591, manual for-loop: 2.30591
❌ [fmnist_vae, l2_reg: True, BackPACK: False] Raised exception: vae_loss_function() missing 2 required positional arguments: 'mean' and 'std_dev'
✓ [mnist_2c2d, l2_reg: True, BackPACK: False] DeepOBS: 2.35603, manual for-loop: 2.35603
✓ [mnist_logreg, l2_reg: True, BackPACK: False] DeepOBS: 2.30259, manual for-loop: 2.30259
✓ [mnist_mlp, l2_reg: True, BackPACK: False] DeepOBS: 2.29524, manual for-loop: 2.29524
❌ [mnist_vae, l2_reg: True, BackPACK: False] Raised exception: vae_loss_function() missing 2 required positional arguments: 'mean' and 'std_dev'
✓ [quadratic_deep, l2_reg: True, BackPACK: False] DeepOBS: 6.66879, manual for-loop: 6.66879
❌ [svhn_3c3d, l2_reg: True, BackPACK: False] DeepOBS: 3.48170, manual for-loop: 2.21970, BatchNorm? False, Dropout? False
❌ [svhn_wrn164, l2_reg: True, BackPACK: False] DeepOBS: 2.37303, manual for-loop: 1.84587, BatchNorm? True, Dropout? False


✓ [cifar10_3c3d, l2_reg: False, BackPACK: True] DeepOBS: 2.28687, manual for-loop: 2.28687
❌ [cifar10_vgg16, l2_reg: False, BackPACK: True] DeepOBS: 2.30151, manual for-loop: 2.30058, BatchNorm? False, Dropout? True
❌ [cifar10_vgg19, l2_reg: False, BackPACK: True] DeepOBS: 2.30262, manual for-loop: 2.30326, BatchNorm? False, Dropout? True
✓ [cifar100_3c3d, l2_reg: False, BackPACK: True] DeepOBS: 4.55693, manual for-loop: 4.55693
❌ [cifar100_allcnnc, l2_reg: False, BackPACK: True] DeepOBS: 4.56741, manual for-loop: 4.56287, BatchNorm? False, Dropout? True
❌ [cifar100_vgg16, l2_reg: False, BackPACK: True] DeepOBS: 4.60366, manual for-loop: 4.60409, BatchNorm? False, Dropout? True
❌ [cifar100_vgg19, l2_reg: False, BackPACK: True] DeepOBS: 4.60555, manual for-loop: 4.60602, BatchNorm? False, Dropout? True
❌ [cifar100_wrn164, l2_reg: False, BackPACK: True] DeepOBS: 4.31506, manual for-loop: 4.37367, BatchNorm? True, Dropout? False
❌ [cifar100_wrn404, l2_reg: False, BackPACK: True] DeepOBS: 4.61947, manual for-loop: 4.40666, BatchNorm? True, Dropout? False
✓ [fmnist_2c2d, l2_reg: False, BackPACK: True] DeepOBS: 2.32473, manual for-loop: 2.32473
✓ [fmnist_logreg, l2_reg: False, BackPACK: True] DeepOBS: 2.30259, manual for-loop: 2.30259
✓ [fmnist_mlp, l2_reg: False, BackPACK: True] DeepOBS: 2.30591, manual for-loop: 2.30591
❌ [fmnist_vae, l2_reg: False, BackPACK: True] Raised exception: 'tuple' object has no attribute 'size'
✓ [mnist_2c2d, l2_reg: False, BackPACK: True] DeepOBS: 2.35603, manual for-loop: 2.35603
✓ [mnist_logreg, l2_reg: False, BackPACK: True] DeepOBS: 2.30259, manual for-loop: 2.30259
✓ [mnist_mlp, l2_reg: False, BackPACK: True] DeepOBS: 2.29524, manual for-loop: 2.29524
❌ [mnist_vae, l2_reg: False, BackPACK: True] Raised exception: 'tuple' object has no attribute 'size'
✓ [quadratic_deep, l2_reg: False, BackPACK: True] DeepOBS: 6.66879, manual for-loop: 6.66879
✓ [svhn_3c3d, l2_reg: False, BackPACK: True] DeepOBS: 2.21970, manual for-loop: 2.21970
❌ [svhn_wrn164, l2_reg: False, BackPACK: True] DeepOBS: 1.89063, manual for-loop: 1.84587, BatchNorm? True, Dropout? False


❌ [cifar10_3c3d, l2_reg: True, BackPACK: True] DeepOBS: 3.54886, manual for-loop: 2.28687, BatchNorm? False, Dropout? False
❌ [cifar10_vgg16, l2_reg: True, BackPACK: True] DeepOBS: 6.05709, manual for-loop: 2.30058, BatchNorm? False, Dropout? True
❌ [cifar10_vgg19, l2_reg: True, BackPACK: True] DeepOBS: 6.37784, manual for-loop: 2.30326, BatchNorm? False, Dropout? True
❌ [cifar100_3c3d, l2_reg: True, BackPACK: True] DeepOBS: 5.94544, manual for-loop: 4.55693, BatchNorm? False, Dropout? False
❌ [cifar100_allcnnc, l2_reg: True, BackPACK: True] DeepOBS: 4.87410, manual for-loop: 4.56287, BatchNorm? False, Dropout? True
❌ [cifar100_vgg16, l2_reg: True, BackPACK: True] DeepOBS: 8.40309, manual for-loop: 4.60409, BatchNorm? False, Dropout? True
❌ [cifar100_vgg19, l2_reg: True, BackPACK: True] DeepOBS: 8.72502, manual for-loop: 4.60602, BatchNorm? False, Dropout? True
❌ [cifar100_wrn164, l2_reg: True, BackPACK: True] DeepOBS: 4.82936, manual for-loop: 4.37367, BatchNorm? True, Dropout? False
❌ [cifar100_wrn404, l2_reg: True, BackPACK: True] Raised exception: 'NoneType' object has no attribute 'items'
✓ [fmnist_2c2d, l2_reg: True, BackPACK: True] DeepOBS: 2.32473, manual for-loop: 2.32473
✓ [fmnist_logreg, l2_reg: True, BackPACK: True] DeepOBS: 2.30259, manual for-loop: 2.30259
✓ [fmnist_mlp, l2_reg: True, BackPACK: True] DeepOBS: 2.30591, manual for-loop: 2.30591
❌ [fmnist_vae, l2_reg: True, BackPACK: True] Raised exception: 'tuple' object has no attribute 'size'
✓ [mnist_2c2d, l2_reg: True, BackPACK: True] DeepOBS: 2.35603, manual for-loop: 2.35603
✓ [mnist_logreg, l2_reg: True, BackPACK: True] DeepOBS: 2.30259, manual for-loop: 2.30259
✓ [mnist_mlp, l2_reg: True, BackPACK: True] DeepOBS: 2.29524, manual for-loop: 2.29524
❌ [mnist_vae, l2_reg: True, BackPACK: True] Raised exception: 'tuple' object has no attribute 'size'
✓ [quadratic_deep, l2_reg: True, BackPACK: True] DeepOBS: 6.66879, manual for-loop: 6.66879
❌ [svhn_3c3d, l2_reg: True, BackPACK: True] DeepOBS: 3.48170, manual for-loop: 2.21970, BatchNorm? False, Dropout? False
❌ [svhn_wrn164, l2_reg: True, BackPACK: True] DeepOBS: 2.37303, manual for-loop: 1.84587, BatchNorm? True, Dropout? False


#+end_example

Comments:
- [X] *BatchNorm unsupported*: The ~cifar100_wrn164~, ~cifar100_wrn404~, ~svhn_wrn164~ problems come with ~BatchNorm~ and can thus not be supported.
- [ ] *Dropout fails test, but* this is just because the order of the two forward passes is different, which leads to different elements being dropped. In principle, it should be able to support the nets with ~Dropout~, i.e. ~cifar100_vgg16~, ~cifar100_vgg19~, ~cifar10_vgg16~, ~cifar10_vgg19~, ~cifar100_allcnnc~.
*** Checking individual gradients
Here is an overview, for which [[https://github.com/fsschneider/DeepOBS][DeepOBS]] testproblems [[https://www.backpack.pt][BackPACK]] correctly computes individual gradients.

To reproduce the results, run
#+begin_src bash :results output
  python test/run_test_batch_grad.py
#+end_src

#+RESULTS:
#+begin_example
✓ [cifar10_3c3d, individual gradients] Same? 12/12
❌ [cifar10_vgg16, individual gradients] Same? 0/32, BatchNorm? False, Dropout? True
❌ [cifar10_vgg19, individual gradients] Same? 0/38, BatchNorm? False, Dropout? True
✓ [cifar100_3c3d, individual gradients] Same? 12/12
❌ [cifar100_allcnnc, individual gradients] Same? 0/18, BatchNorm? False, Dropout? True
❌ [cifar100_vgg16, individual gradients] Same? 0/32, BatchNorm? False, Dropout? True
❌ [cifar100_vgg19, individual gradients] Same? 0/38, BatchNorm? False, Dropout? True
❌ [cifar100_wrn164, individual gradients] Raised exception: 'Parameter' object has no attribute 'grad_batch'
❌ [cifar100_wrn404, individual gradients] Raised exception: 'Parameter' object has no attribute 'grad_batch'
✓ [fmnist_2c2d, individual gradients] Same? 8/8
✓ [fmnist_logreg, individual gradients] Same? 2/2
✓ [fmnist_mlp, individual gradients] Same? 8/8
❌ [fmnist_vae, individual gradients] Raised exception: 'function' object has no attribute 'children'
✓ [mnist_2c2d, individual gradients] Same? 8/8
✓ [mnist_logreg, individual gradients] Same? 2/2
✓ [mnist_mlp, individual gradients] Same? 8/8
❌ [mnist_vae, individual gradients] Raised exception: 'function' object has no attribute 'children'
✓ [quadratic_deep, individual gradients] Same? 1/1
✓ [svhn_3c3d, individual gradients] Same? 12/12
❌ [svhn_wrn164, individual gradients] Raised exception: 'Parameter' object has no attribute 'grad_batch'
#+end_example

Notes:
- [ ] Networks with ~Dropout~ do not have a deterministic forward pass. As such, the individual gradients are not identical, but in principle, it [[https://www.backpack.pt][BackPACK]] should still work.
- [X] Individual gradients are not properly defined for networks with batch normalization. This is expected.
- [X] Variational autoencoders are not supported in [[https://www.backpack.pt][BackPACK]]. This is expected.
- [X] For wide ResNets, the [[https://www.backpack.pt][BackPACK]] computation does not seem to work. But, they also come with ~BatchNorm~, and hence individual gradients are not well-defined.
**** TODO [[https://www.backpack.pt][BackPACK]] is known to work with ResNets. Can we find a popular ResNet without ~BatchNorm~ and integrate it into [[https://github.com/fsschneider/DeepOBS][DeepOBS]]?
** Extracting the model forward pass
- Goal :: Evaluating the loss with ~reduction="mean"~ should at the same time give access to the unreduced (individual) losses.
- How  :: The forward pass usually consists of two stages:
  1. Forward pass through the network: ~output = model(X)~
  2. Evaluation of loss: ~loss = loss_function(output, y)~
  
  The idea is to perform the forward pass through the model only once, and compute the second step with different reductions.
- Backround  :: For the cockpit, we need both the reduced and unreduced loss. But we cannot manually reduce the individual losses, as this breaks the computation of second-order extensions in [[https://www.backpack.pt][BackPACK]].
